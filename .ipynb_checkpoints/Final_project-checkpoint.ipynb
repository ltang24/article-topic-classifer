{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bc076313",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "\n",
    "# dataframe = pd.read_csv('train_set_small.csv', sep='\\t')\n",
    "# test_dataframe = pd.read_csv('bbc-news-data.csv', sep='\\t')\n",
    "# A = np.array(dataframe)\n",
    "# length = A.shape[0]\n",
    "# print(\"Size of input: \", length)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed090f57-340d-4374-add1-7c4686ab1f23",
   "metadata": {},
   "source": [
    "# use anaconda or basic package manager for installing base packages @all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "33a3aca2-fb6b-4776-9b43-22193d8b0061",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in c:\\users\\sunyloaner\\anaconda3\\lib\\site-packages (2.0.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\sunyloaner\\anaconda3\\lib\\site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\sunyloaner\\anaconda3\\lib\\site-packages (from pandas) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in c:\\users\\sunyloaner\\anaconda3\\lib\\site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: numpy>=1.21.0 in c:\\users\\sunyloaner\\anaconda3\\lib\\site-packages (from pandas) (1.24.3)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\sunyloaner\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "07ebd592-642f-44f2-871a-ee3430b13343",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in c:\\users\\sunyloaner\\anaconda3\\lib\\site-packages (1.24.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "25ba35a5-3ea5-4369-b5d6-ed037828878d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting package metadata (current_repodata.json): ...working... done\n",
      "Solving environment: ...working... done\n",
      "\n",
      "# All requested packages already installed.\n",
      "\n",
      "\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "==> WARNING: A newer version of conda exists. <==\n",
      "  current version: 23.7.4\n",
      "  latest version: 23.11.0\n",
      "\n",
      "Please update conda by running\n",
      "\n",
      "    $ conda update -n base -c defaults conda\n",
      "\n",
      "Or to minimize the number of packages updated during conda update use\n",
      "\n",
      "     conda install conda=23.11.0\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "conda install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "132d7e72-71dc-4f7d-9baa-fa8af26f1989",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting package metadata (current_repodata.json): ...working... done\n",
      "Solving environment: ...working... done\n",
      "\n",
      "# All requested packages already installed.\n",
      "\n",
      "\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "==> WARNING: A newer version of conda exists. <==\n",
      "  current version: 23.7.4\n",
      "  latest version: 23.11.0\n",
      "\n",
      "Please update conda by running\n",
      "\n",
      "    $ conda update -n base -c defaults conda\n",
      "\n",
      "Or to minimize the number of packages updated during conda update use\n",
      "\n",
      "     conda install conda=23.11.0\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "conda install wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1ffdeff9-b694-4bda-b28b-c48a2f49b147",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: seaborn in c:\\users\\sunyloaner\\anaconda3\\lib\\site-packages (0.12.2)\n",
      "Requirement already satisfied: numpy!=1.24.0,>=1.17 in c:\\users\\sunyloaner\\anaconda3\\lib\\site-packages (from seaborn) (1.24.3)\n",
      "Requirement already satisfied: pandas>=0.25 in c:\\users\\sunyloaner\\anaconda3\\lib\\site-packages (from seaborn) (2.0.3)\n",
      "Requirement already satisfied: matplotlib!=3.6.1,>=3.1 in c:\\users\\sunyloaner\\anaconda3\\lib\\site-packages (from seaborn) (3.7.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\sunyloaner\\anaconda3\\lib\\site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (1.0.5)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\sunyloaner\\anaconda3\\lib\\site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\sunyloaner\\anaconda3\\lib\\site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (4.25.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\sunyloaner\\anaconda3\\lib\\site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (1.4.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\sunyloaner\\anaconda3\\lib\\site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (23.1)\n",
      "Requirement already satisfied: pillow>=6.2.0 in c:\\users\\sunyloaner\\anaconda3\\lib\\site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (10.0.1)\n",
      "Requirement already satisfied: pyparsing<3.1,>=2.3.1 in c:\\users\\sunyloaner\\anaconda3\\lib\\site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (3.0.9)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\sunyloaner\\anaconda3\\lib\\site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\sunyloaner\\anaconda3\\lib\\site-packages (from pandas>=0.25->seaborn) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in c:\\users\\sunyloaner\\anaconda3\\lib\\site-packages (from pandas>=0.25->seaborn) (2023.3)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\sunyloaner\\anaconda3\\lib\\site-packages (from python-dateutil>=2.7->matplotlib!=3.6.1,>=3.1->seaborn) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install seaborn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a83263ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c96b2702",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = pd.read_csv('bbc-news-data.csv', sep='\\t', header=None, names=['Category','Filename','Title','Content'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4b5b299d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b9de4c03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e465d1ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data.nunique().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d607c01f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data['Category'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a8e1416f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2225 entries, 0 to 2224\n",
      "Data columns (total 4 columns):\n",
      " #   Column    Non-Null Count  Dtype \n",
      "---  ------    --------------  ----- \n",
      " 0   category  2225 non-null   object\n",
      " 1   filename  2225 non-null   object\n",
      " 2   title     2225 non-null   object\n",
      " 3   content   2225 non-null   object\n",
      "dtypes: object(4)\n",
      "memory usage: 69.7+ KB\n",
      "None \n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\SUNYLoaner\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\SUNYLoaner\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "No objects to concatenate",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[36], line 125\u001b[0m\n\u001b[0;32m    122\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m lemmatizer(stopword(preprocess(string)))\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    124\u001b[0m     \u001b[38;5;66;03m#print(train_dataframe.head)\u001b[39;00m\n\u001b[1;32m--> 125\u001b[0m     EDA(dataframe,train_dataframe,test_dataframe)\n",
      "Cell \u001b[1;32mIn[36], line 33\u001b[0m, in \u001b[0;36mEDA\u001b[1;34m(dataframe, train_dataframe, test_dataframe)\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mEDA\u001b[39m(dataframe,train_dataframe,test_dataframe):\n\u001b[0;32m     32\u001b[0m     \u001b[38;5;28mprint\u001b[39m(dataframe\u001b[38;5;241m.\u001b[39minfo(),\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 33\u001b[0m     df_numeric\u001b[38;5;241m=\u001b[39mdataframe\u001b[38;5;241m.\u001b[39mdescribe(include\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mnumber)\n\u001b[0;32m     34\u001b[0m     \u001b[38;5;28mprint\u001b[39m(df_numeric,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     37\u001b[0m     x\u001b[38;5;241m=\u001b[39mdataframe[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCategory\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mvalue_counts()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\generic.py:10819\u001b[0m, in \u001b[0;36mNDFrame.describe\u001b[1;34m(self, percentiles, include, exclude)\u001b[0m\n\u001b[0;32m  10577\u001b[0m \u001b[38;5;129m@final\u001b[39m\n\u001b[0;32m  10578\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdescribe\u001b[39m(\n\u001b[0;32m  10579\u001b[0m     \u001b[38;5;28mself\u001b[39m: NDFrameT,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m  10582\u001b[0m     exclude\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m  10583\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m NDFrameT:\n\u001b[0;32m  10584\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m  10585\u001b[0m \u001b[38;5;124;03m    Generate descriptive statistics.\u001b[39;00m\n\u001b[0;32m  10586\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m  10817\u001b[0m \u001b[38;5;124;03m    max            NaN      3.0\u001b[39;00m\n\u001b[0;32m  10818\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m> 10819\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m describe_ndframe(\n\u001b[0;32m  10820\u001b[0m         obj\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m  10821\u001b[0m         include\u001b[38;5;241m=\u001b[39minclude,\n\u001b[0;32m  10822\u001b[0m         exclude\u001b[38;5;241m=\u001b[39mexclude,\n\u001b[0;32m  10823\u001b[0m         percentiles\u001b[38;5;241m=\u001b[39mpercentiles,\n\u001b[0;32m  10824\u001b[0m     )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\methods\\describe.py:94\u001b[0m, in \u001b[0;36mdescribe_ndframe\u001b[1;34m(obj, include, exclude, percentiles)\u001b[0m\n\u001b[0;32m     87\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     88\u001b[0m     describer \u001b[38;5;241m=\u001b[39m DataFrameDescriber(\n\u001b[0;32m     89\u001b[0m         obj\u001b[38;5;241m=\u001b[39mcast(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataFrame\u001b[39m\u001b[38;5;124m\"\u001b[39m, obj),\n\u001b[0;32m     90\u001b[0m         include\u001b[38;5;241m=\u001b[39minclude,\n\u001b[0;32m     91\u001b[0m         exclude\u001b[38;5;241m=\u001b[39mexclude,\n\u001b[0;32m     92\u001b[0m     )\n\u001b[1;32m---> 94\u001b[0m result \u001b[38;5;241m=\u001b[39m describer\u001b[38;5;241m.\u001b[39mdescribe(percentiles\u001b[38;5;241m=\u001b[39mpercentiles)\n\u001b[0;32m     95\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m cast(NDFrameT, result)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\methods\\describe.py:170\u001b[0m, in \u001b[0;36mDataFrameDescriber.describe\u001b[1;34m(self, percentiles)\u001b[0m\n\u001b[0;32m    167\u001b[0m     ldesc\u001b[38;5;241m.\u001b[39mappend(describe_func(series, percentiles))\n\u001b[0;32m    169\u001b[0m col_names \u001b[38;5;241m=\u001b[39m reorder_columns(ldesc)\n\u001b[1;32m--> 170\u001b[0m d \u001b[38;5;241m=\u001b[39m concat(\n\u001b[0;32m    171\u001b[0m     [x\u001b[38;5;241m.\u001b[39mreindex(col_names, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m ldesc],\n\u001b[0;32m    172\u001b[0m     axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[0;32m    173\u001b[0m     sort\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    174\u001b[0m )\n\u001b[0;32m    175\u001b[0m d\u001b[38;5;241m.\u001b[39mcolumns \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[0;32m    176\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m d\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\reshape\\concat.py:372\u001b[0m, in \u001b[0;36mconcat\u001b[1;34m(objs, axis, join, ignore_index, keys, levels, names, verify_integrity, sort, copy)\u001b[0m\n\u001b[0;32m    369\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m copy \u001b[38;5;129;01mand\u001b[39;00m using_copy_on_write():\n\u001b[0;32m    370\u001b[0m     copy \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m--> 372\u001b[0m op \u001b[38;5;241m=\u001b[39m _Concatenator(\n\u001b[0;32m    373\u001b[0m     objs,\n\u001b[0;32m    374\u001b[0m     axis\u001b[38;5;241m=\u001b[39maxis,\n\u001b[0;32m    375\u001b[0m     ignore_index\u001b[38;5;241m=\u001b[39mignore_index,\n\u001b[0;32m    376\u001b[0m     join\u001b[38;5;241m=\u001b[39mjoin,\n\u001b[0;32m    377\u001b[0m     keys\u001b[38;5;241m=\u001b[39mkeys,\n\u001b[0;32m    378\u001b[0m     levels\u001b[38;5;241m=\u001b[39mlevels,\n\u001b[0;32m    379\u001b[0m     names\u001b[38;5;241m=\u001b[39mnames,\n\u001b[0;32m    380\u001b[0m     verify_integrity\u001b[38;5;241m=\u001b[39mverify_integrity,\n\u001b[0;32m    381\u001b[0m     copy\u001b[38;5;241m=\u001b[39mcopy,\n\u001b[0;32m    382\u001b[0m     sort\u001b[38;5;241m=\u001b[39msort,\n\u001b[0;32m    383\u001b[0m )\n\u001b[0;32m    385\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m op\u001b[38;5;241m.\u001b[39mget_result()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\reshape\\concat.py:429\u001b[0m, in \u001b[0;36m_Concatenator.__init__\u001b[1;34m(self, objs, axis, join, keys, levels, names, ignore_index, verify_integrity, copy, sort)\u001b[0m\n\u001b[0;32m    426\u001b[0m     objs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(objs)\n\u001b[0;32m    428\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(objs) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m--> 429\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo objects to concatenate\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    431\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m keys \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    432\u001b[0m     objs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(com\u001b[38;5;241m.\u001b[39mnot_none(\u001b[38;5;241m*\u001b[39mobjs))\n",
      "\u001b[1;31mValueError\u001b[0m: No objects to concatenate"
     ]
    }
   ],
   "source": [
    "# have to implement this on bbc dataset\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns  \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#for text pre-processing\n",
    "import re, string\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "#for model-building\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "\n",
    "#LEMMATIZATION\n",
    "# Initialize the lemmatizer\n",
    "wl = WordNetLemmatizer()\n",
    "\n",
    "dataframe = pd.read_csv('bbc-news-data.csv', sep='\\t')\n",
    "train_dataframe, test_dataframe = train_test_split(dataframe, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "def EDA(dataframe,train_dataframe,test_dataframe):\n",
    "    print(dataframe.info(),\"\\n\\n\")\n",
    "    df_numeric=dataframe.describe(include=np.number)\n",
    "    print(df_numeric,\"\\n\")\n",
    "    \n",
    "    \n",
    "    x=dataframe[\"Category\"].value_counts()\n",
    "    #print(x)\n",
    "    sns.barplot(x=x.index, y=x.values)\n",
    "    plt.xlabel(\"Category\")\n",
    "    plt.ylabel(\"Count\")\n",
    "    plt.title(\"Value Counts of Categories total data\")\n",
    "    plt.show()\n",
    "    #test data\n",
    "    x=train_dataframe[\"Category\"].value_counts()\n",
    "    sns.barplot(x=x.index, y=x.values)\n",
    "    plt.xlabel(\"Category\")\n",
    "    plt.ylabel(\"Count\")\n",
    "    plt.title(\"Value Counts of Categories train\")\n",
    "    plt.show()\n",
    "\n",
    "    #train data\n",
    "    x=test_dataframe[\"Category\"].value_counts()\n",
    "    sns.barplot(x=x.index, y=x.values)\n",
    "    plt.xlabel(\"Category\")\n",
    "    plt.ylabel(\"Count\")\n",
    "    plt.title(\"Value Counts of Categories test\")\n",
    "    plt.show()\n",
    "    #missing values \n",
    "    print(\"Missing Value Count :\")\n",
    "    print(dataframe.isna().sum())\n",
    "    #mean word count\n",
    "    # WORD-COUNT\n",
    "    dataframe['word_count'] = dataframe['Content'].apply(lambda x: len(str(x).split()))\n",
    "    print(dataframe.head)\n",
    "    print(\"\\nMean word count of Football Article: \",dataframe[dataframe['Category']=='business']['word_count'].mean()) #Disaster tweets\n",
    "    print(\"Mean word count of Business Article: \",dataframe[dataframe['Category']=='entertainment']['word_count'].mean()) #Non-Disaster tweets\n",
    "    print(\"Mean word count of Politics Article: \",dataframe[dataframe['Category']=='politics']['word_count'].mean())\n",
    "    print(\"Mean word count of Film Article: \",dataframe[dataframe['Category']=='sport']['word_count'].mean())\n",
    "    print(\"Mean word count of Technology Article: \",dataframe[dataframe['Category']=='tech']['word_count'].mean())\n",
    "\n",
    "\n",
    "#convert to lowercase, strip and remove punctuations\n",
    "def preprocess(Content):\n",
    "    # lowercase of letters\n",
    "    Content = Content.lower() \n",
    "    #strip spaces\n",
    "    Content=Content.strip()  \n",
    "    #get all the special symbols\n",
    "    Content=re.compile('<.*?>').sub('', Content) \n",
    "    #get format characters\n",
    "    Content = re.compile('[%s]' % re.escape(string.punctuation)).sub(' ', Content)  \n",
    "    #substitutions\n",
    "    Content = re.sub('\\s+', ' ', Content)  \n",
    "    Content = re.sub(r'\\[[0-9]*\\]',' ',Content) \n",
    "    Content=re.sub(r'[^\\w\\s]', '', str(Content).lower().strip())\n",
    "    Content = re.sub(r'\\d',' ',Content) \n",
    "    Content = re.sub(r'\\s+',' ',Content) \n",
    "    return Content\n",
    "\n",
    " \n",
    "# STOPWORD REMOVAL\n",
    "def stopword(string):\n",
    "    a= [i for i in string.split() if i not in stopwords.words('english')]\n",
    "    return ' '.join(a)\n",
    "\n",
    " \n",
    "# This is a helper function to map NTLK position tags\n",
    "\"\"\"WordNet is a large lexical database of English words that can be used to determine the lemma of a word. NLTK provides a WordNet lemmatizer that can be used to lemmatize words based on their part of speech (POS).\"\"\"\n",
    "def get_wordnet_pos(tag):\n",
    "    if tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN\n",
    "# Tokenize the sentence\n",
    "\"\"\"\n",
    "A lemmatizer is a natural language processing (NLP) tool that reduces words to their base forms, also known as lemmas. This process is called lemmatization. Lemmatization is similar to stemming, but it is more sophisticated and takes into account the context of the word to determine the correct lemma.\n",
    "\n",
    "For example, the lemma of the word \"running\" is \"run\". This is because \"running\" is a verb form of the word \"run\". Similarly, the lemma of the word \"saw\" is \"see\". This is because \"saw\" is a past tense form of the word \"see\".\"\"\"\n",
    "def lemmatizer(string):\n",
    "    word_pos_tags = nltk.pos_tag(word_tokenize(string)) # Get position tags\n",
    "    a=[wl.lemmatize(tag[0], get_wordnet_pos(tag[1])) for idx, tag in enumerate(word_pos_tags)] # Map the position tag and lemmatize the word/token\n",
    "    #print (\"\\n\\n\\n\\n\" ,a, \"\\n\\n\\n\\n\")\n",
    "    return \" \".join(a)\n",
    "def finalpreprocess(string):\n",
    "    return lemmatizer(stopword(preprocess(string)))\n",
    "if __name__ == \"__main__\":\n",
    "    #print(train_dataframe.head)\n",
    "    EDA(dataframe,train_dataframe,test_dataframe)\n",
    "#     train_dataframe['clean_text'] = train_dataframe['Content'].apply(lambda x: finalpreprocess(x))\n",
    "#     #print(train_dataframe.head)\n",
    "#     train_dataframe.to_csv('testclean.csv',sep='\\t',index=False)\n",
    "#     train_dataframe['word_count'] = train_dataframe['clean_text'].apply(lambda x: len(str(x).split()))\n",
    "#     #print(dataframe.head)\n",
    "#     print(\"\\nMean word count of cleaned Football Article: \",train_dataframe[train_dataframe['Category']=='business']['word_count'].mean()) #Disaster tweets\n",
    "#     print(\"Mean word count of cleaned Business Article: \",train_dataframe[train_dataframe['Category']=='entertainment']['word_count'].mean()) #Non-Disaster tweets\n",
    "#     print(\"Mean word count of cleaned Politics Article: \",train_dataframe[train_dataframe['Category']=='politics']['word_count'].mean())\n",
    "#     print(\"Mean word count of cleaned Film Article: \",train_dataframe[train_dataframe['Category']=='sport']['word_count'].mean())\n",
    "#     print(\"Mean word count of cleaned Technology Article: \",train_dataframe[train_dataframe['Category']=='tech']['word_count'].mean())\n",
    "    \n",
    "    \n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8921d57d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
