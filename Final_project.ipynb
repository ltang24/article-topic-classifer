{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bc076313",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "\n",
    "# dataframe = pd.read_csv('train_set_small.csv', sep='\\t')\n",
    "# test_dataframe = pd.read_csv('bbc-news-data.csv', sep='\\t')\n",
    "# A = np.array(dataframe)\n",
    "# length = A.shape[0]\n",
    "# print(\"Size of input: \", length)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed090f57-340d-4374-add1-7c4686ab1f23",
   "metadata": {},
   "source": [
    "# use anaconda or basic package manager for installing base packages @all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "33a3aca2-fb6b-4776-9b43-22193d8b0061",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in c:\\users\\sunyloaner\\anaconda3\\lib\\site-packages (2.0.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\sunyloaner\\anaconda3\\lib\\site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\sunyloaner\\anaconda3\\lib\\site-packages (from pandas) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in c:\\users\\sunyloaner\\anaconda3\\lib\\site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: numpy>=1.21.0 in c:\\users\\sunyloaner\\anaconda3\\lib\\site-packages (from pandas) (1.24.3)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\sunyloaner\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "07ebd592-642f-44f2-871a-ee3430b13343",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in c:\\users\\sunyloaner\\anaconda3\\lib\\site-packages (1.24.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "25ba35a5-3ea5-4369-b5d6-ed037828878d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting package metadata (current_repodata.json): ...working... done\n",
      "Solving environment: ...working... done\n",
      "\n",
      "# All requested packages already installed.\n",
      "\n",
      "\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "==> WARNING: A newer version of conda exists. <==\n",
      "  current version: 23.7.4\n",
      "  latest version: 23.11.0\n",
      "\n",
      "Please update conda by running\n",
      "\n",
      "    $ conda update -n base -c defaults conda\n",
      "\n",
      "Or to minimize the number of packages updated during conda update use\n",
      "\n",
      "     conda install conda=23.11.0\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "conda install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "132d7e72-71dc-4f7d-9baa-fa8af26f1989",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting package metadata (current_repodata.json): ...working... done\n",
      "Solving environment: ...working... done\n",
      "\n",
      "# All requested packages already installed.\n",
      "\n",
      "\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "==> WARNING: A newer version of conda exists. <==\n",
      "  current version: 23.7.4\n",
      "  latest version: 23.11.0\n",
      "\n",
      "Please update conda by running\n",
      "\n",
      "    $ conda update -n base -c defaults conda\n",
      "\n",
      "Or to minimize the number of packages updated during conda update use\n",
      "\n",
      "     conda install conda=23.11.0\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "conda install wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1ffdeff9-b694-4bda-b28b-c48a2f49b147",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: seaborn in c:\\users\\sunyloaner\\anaconda3\\lib\\site-packages (0.12.2)\n",
      "Requirement already satisfied: numpy!=1.24.0,>=1.17 in c:\\users\\sunyloaner\\anaconda3\\lib\\site-packages (from seaborn) (1.24.3)\n",
      "Requirement already satisfied: pandas>=0.25 in c:\\users\\sunyloaner\\anaconda3\\lib\\site-packages (from seaborn) (2.0.3)\n",
      "Requirement already satisfied: matplotlib!=3.6.1,>=3.1 in c:\\users\\sunyloaner\\anaconda3\\lib\\site-packages (from seaborn) (3.7.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\sunyloaner\\anaconda3\\lib\\site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (1.0.5)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\sunyloaner\\anaconda3\\lib\\site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\sunyloaner\\anaconda3\\lib\\site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (4.25.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\sunyloaner\\anaconda3\\lib\\site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (1.4.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\sunyloaner\\anaconda3\\lib\\site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (23.1)\n",
      "Requirement already satisfied: pillow>=6.2.0 in c:\\users\\sunyloaner\\anaconda3\\lib\\site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (10.0.1)\n",
      "Requirement already satisfied: pyparsing<3.1,>=2.3.1 in c:\\users\\sunyloaner\\anaconda3\\lib\\site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (3.0.9)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\sunyloaner\\anaconda3\\lib\\site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\sunyloaner\\anaconda3\\lib\\site-packages (from pandas>=0.25->seaborn) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in c:\\users\\sunyloaner\\anaconda3\\lib\\site-packages (from pandas>=0.25->seaborn) (2023.3)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\sunyloaner\\anaconda3\\lib\\site-packages (from python-dateutil>=2.7->matplotlib!=3.6.1,>=3.1->seaborn) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install seaborn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a83263ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c96b2702",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = pd.read_csv('bbc-news-data.csv', sep='\\t', header=None, names=['Category','Filename','Title','Content'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4b5b299d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b9de4c03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e465d1ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data.nunique().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d607c01f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data['Category'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a8e1416f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2225 entries, 0 to 2224\n",
      "Data columns (total 4 columns):\n",
      " #   Column    Non-Null Count  Dtype \n",
      "---  ------    --------------  ----- \n",
      " 0   category  2225 non-null   object\n",
      " 1   filename  2225 non-null   object\n",
      " 2   title     2225 non-null   object\n",
      " 3   content   2225 non-null   object\n",
      "dtypes: object(4)\n",
      "memory usage: 69.7+ KB\n",
      "None \n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\SUNYLoaner\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\SUNYLoaner\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "No objects to concatenate",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[36], line 125\u001b[0m\n\u001b[0;32m    122\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m lemmatizer(stopword(preprocess(string)))\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    124\u001b[0m     \u001b[38;5;66;03m#print(train_dataframe.head)\u001b[39;00m\n\u001b[1;32m--> 125\u001b[0m     EDA(dataframe,train_dataframe,test_dataframe)\n",
      "Cell \u001b[1;32mIn[36], line 33\u001b[0m, in \u001b[0;36mEDA\u001b[1;34m(dataframe, train_dataframe, test_dataframe)\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mEDA\u001b[39m(dataframe,train_dataframe,test_dataframe):\n\u001b[0;32m     32\u001b[0m     \u001b[38;5;28mprint\u001b[39m(dataframe\u001b[38;5;241m.\u001b[39minfo(),\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 33\u001b[0m     df_numeric\u001b[38;5;241m=\u001b[39mdataframe\u001b[38;5;241m.\u001b[39mdescribe(include\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mnumber)\n\u001b[0;32m     34\u001b[0m     \u001b[38;5;28mprint\u001b[39m(df_numeric,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     37\u001b[0m     x\u001b[38;5;241m=\u001b[39mdataframe[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCategory\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mvalue_counts()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\generic.py:10819\u001b[0m, in \u001b[0;36mNDFrame.describe\u001b[1;34m(self, percentiles, include, exclude)\u001b[0m\n\u001b[0;32m  10577\u001b[0m \u001b[38;5;129m@final\u001b[39m\n\u001b[0;32m  10578\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdescribe\u001b[39m(\n\u001b[0;32m  10579\u001b[0m     \u001b[38;5;28mself\u001b[39m: NDFrameT,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m  10582\u001b[0m     exclude\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m  10583\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m NDFrameT:\n\u001b[0;32m  10584\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m  10585\u001b[0m \u001b[38;5;124;03m    Generate descriptive statistics.\u001b[39;00m\n\u001b[0;32m  10586\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m  10817\u001b[0m \u001b[38;5;124;03m    max            NaN      3.0\u001b[39;00m\n\u001b[0;32m  10818\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m> 10819\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m describe_ndframe(\n\u001b[0;32m  10820\u001b[0m         obj\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m  10821\u001b[0m         include\u001b[38;5;241m=\u001b[39minclude,\n\u001b[0;32m  10822\u001b[0m         exclude\u001b[38;5;241m=\u001b[39mexclude,\n\u001b[0;32m  10823\u001b[0m         percentiles\u001b[38;5;241m=\u001b[39mpercentiles,\n\u001b[0;32m  10824\u001b[0m     )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\methods\\describe.py:94\u001b[0m, in \u001b[0;36mdescribe_ndframe\u001b[1;34m(obj, include, exclude, percentiles)\u001b[0m\n\u001b[0;32m     87\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     88\u001b[0m     describer \u001b[38;5;241m=\u001b[39m DataFrameDescriber(\n\u001b[0;32m     89\u001b[0m         obj\u001b[38;5;241m=\u001b[39mcast(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataFrame\u001b[39m\u001b[38;5;124m\"\u001b[39m, obj),\n\u001b[0;32m     90\u001b[0m         include\u001b[38;5;241m=\u001b[39minclude,\n\u001b[0;32m     91\u001b[0m         exclude\u001b[38;5;241m=\u001b[39mexclude,\n\u001b[0;32m     92\u001b[0m     )\n\u001b[1;32m---> 94\u001b[0m result \u001b[38;5;241m=\u001b[39m describer\u001b[38;5;241m.\u001b[39mdescribe(percentiles\u001b[38;5;241m=\u001b[39mpercentiles)\n\u001b[0;32m     95\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m cast(NDFrameT, result)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\methods\\describe.py:170\u001b[0m, in \u001b[0;36mDataFrameDescriber.describe\u001b[1;34m(self, percentiles)\u001b[0m\n\u001b[0;32m    167\u001b[0m     ldesc\u001b[38;5;241m.\u001b[39mappend(describe_func(series, percentiles))\n\u001b[0;32m    169\u001b[0m col_names \u001b[38;5;241m=\u001b[39m reorder_columns(ldesc)\n\u001b[1;32m--> 170\u001b[0m d \u001b[38;5;241m=\u001b[39m concat(\n\u001b[0;32m    171\u001b[0m     [x\u001b[38;5;241m.\u001b[39mreindex(col_names, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m ldesc],\n\u001b[0;32m    172\u001b[0m     axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[0;32m    173\u001b[0m     sort\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    174\u001b[0m )\n\u001b[0;32m    175\u001b[0m d\u001b[38;5;241m.\u001b[39mcolumns \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[0;32m    176\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m d\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\reshape\\concat.py:372\u001b[0m, in \u001b[0;36mconcat\u001b[1;34m(objs, axis, join, ignore_index, keys, levels, names, verify_integrity, sort, copy)\u001b[0m\n\u001b[0;32m    369\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m copy \u001b[38;5;129;01mand\u001b[39;00m using_copy_on_write():\n\u001b[0;32m    370\u001b[0m     copy \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m--> 372\u001b[0m op \u001b[38;5;241m=\u001b[39m _Concatenator(\n\u001b[0;32m    373\u001b[0m     objs,\n\u001b[0;32m    374\u001b[0m     axis\u001b[38;5;241m=\u001b[39maxis,\n\u001b[0;32m    375\u001b[0m     ignore_index\u001b[38;5;241m=\u001b[39mignore_index,\n\u001b[0;32m    376\u001b[0m     join\u001b[38;5;241m=\u001b[39mjoin,\n\u001b[0;32m    377\u001b[0m     keys\u001b[38;5;241m=\u001b[39mkeys,\n\u001b[0;32m    378\u001b[0m     levels\u001b[38;5;241m=\u001b[39mlevels,\n\u001b[0;32m    379\u001b[0m     names\u001b[38;5;241m=\u001b[39mnames,\n\u001b[0;32m    380\u001b[0m     verify_integrity\u001b[38;5;241m=\u001b[39mverify_integrity,\n\u001b[0;32m    381\u001b[0m     copy\u001b[38;5;241m=\u001b[39mcopy,\n\u001b[0;32m    382\u001b[0m     sort\u001b[38;5;241m=\u001b[39msort,\n\u001b[0;32m    383\u001b[0m )\n\u001b[0;32m    385\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m op\u001b[38;5;241m.\u001b[39mget_result()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\reshape\\concat.py:429\u001b[0m, in \u001b[0;36m_Concatenator.__init__\u001b[1;34m(self, objs, axis, join, keys, levels, names, ignore_index, verify_integrity, copy, sort)\u001b[0m\n\u001b[0;32m    426\u001b[0m     objs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(objs)\n\u001b[0;32m    428\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(objs) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m--> 429\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo objects to concatenate\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    431\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m keys \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    432\u001b[0m     objs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(com\u001b[38;5;241m.\u001b[39mnot_none(\u001b[38;5;241m*\u001b[39mobjs))\n",
      "\u001b[1;31mValueError\u001b[0m: No objects to concatenate"
     ]
    }
   ],
   "source": [
    "# have to implement this on bbc dataset\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns  \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#for text pre-processing\n",
    "import re, string\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "#for model-building\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "\n",
    "#LEMMATIZATION\n",
    "# Initialize the lemmatizer\n",
    "wl = WordNetLemmatizer()\n",
    "\n",
    "dataframe = pd.read_csv('bbc-news-data.csv', sep='\\t')\n",
    "train_dataframe, test_dataframe = train_test_split(dataframe, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "def EDA(dataframe,train_dataframe,test_dataframe):\n",
    "    print(dataframe.info(),\"\\n\\n\")\n",
    "    df_numeric=dataframe.describe(include=np.number)\n",
    "    print(df_numeric,\"\\n\")\n",
    "    \n",
    "    \n",
    "    x=dataframe[\"Category\"].value_counts()\n",
    "    #print(x)\n",
    "    sns.barplot(x=x.index, y=x.values)\n",
    "    plt.xlabel(\"Category\")\n",
    "    plt.ylabel(\"Count\")\n",
    "    plt.title(\"Value Counts of Categories total data\")\n",
    "    plt.show()\n",
    "    #test data\n",
    "    x=train_dataframe[\"Category\"].value_counts()\n",
    "    sns.barplot(x=x.index, y=x.values)\n",
    "    plt.xlabel(\"Category\")\n",
    "    plt.ylabel(\"Count\")\n",
    "    plt.title(\"Value Counts of Categories train\")\n",
    "    plt.show()\n",
    "\n",
    "    #train data\n",
    "    x=test_dataframe[\"Category\"].value_counts()\n",
    "    sns.barplot(x=x.index, y=x.values)\n",
    "    plt.xlabel(\"Category\")\n",
    "    plt.ylabel(\"Count\")\n",
    "    plt.title(\"Value Counts of Categories test\")\n",
    "    plt.show()\n",
    "    #missing values \n",
    "    print(\"Missing Value Count :\")\n",
    "    print(dataframe.isna().sum())\n",
    "    #mean word count\n",
    "    # WORD-COUNT\n",
    "    dataframe['word_count'] = dataframe['Content'].apply(lambda x: len(str(x).split()))\n",
    "    print(dataframe.head)\n",
    "    print(\"\\nMean word count of Football Article: \",dataframe[dataframe['Category']=='business']['word_count'].mean()) #Disaster tweets\n",
    "    print(\"Mean word count of Business Article: \",dataframe[dataframe['Category']=='entertainment']['word_count'].mean()) #Non-Disaster tweets\n",
    "    print(\"Mean word count of Politics Article: \",dataframe[dataframe['Category']=='politics']['word_count'].mean())\n",
    "    print(\"Mean word count of Film Article: \",dataframe[dataframe['Category']=='sport']['word_count'].mean())\n",
    "    print(\"Mean word count of Technology Article: \",dataframe[dataframe['Category']=='tech']['word_count'].mean())\n",
    "\n",
    "\n",
    "#convert to lowercase, strip and remove punctuations\n",
    "def preprocess(Content):\n",
    "    # lowercase of letters\n",
    "    Content = Content.lower() \n",
    "    #strip spaces\n",
    "    Content=Content.strip()  \n",
    "    #get all the special symbols\n",
    "    Content=re.compile('<.*?>').sub('', Content) \n",
    "    #get format characters\n",
    "    Content = re.compile('[%s]' % re.escape(string.punctuation)).sub(' ', Content)  \n",
    "    #substitutions\n",
    "    Content = re.sub('\\s+', ' ', Content)  \n",
    "    Content = re.sub(r'\\[[0-9]*\\]',' ',Content) \n",
    "    Content=re.sub(r'[^\\w\\s]', '', str(Content).lower().strip())\n",
    "    Content = re.sub(r'\\d',' ',Content) \n",
    "    Content = re.sub(r'\\s+',' ',Content) \n",
    "    return Content\n",
    "\n",
    " \n",
    "# STOPWORD REMOVAL\n",
    "def stopword(string):\n",
    "    a= [i for i in string.split() if i not in stopwords.words('english')]\n",
    "    return ' '.join(a)\n",
    "\n",
    " \n",
    "# This is a helper function to map NTLK position tags\n",
    "\"\"\"WordNet is a large lexical database of English words that can be used to determine the lemma of a word. NLTK provides a WordNet lemmatizer that can be used to lemmatize words based on their part of speech (POS).\"\"\"\n",
    "def get_wordnet_pos(tag):\n",
    "    if tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN\n",
    "# Tokenize the sentence\n",
    "\"\"\"\n",
    "A lemmatizer is a natural language processing (NLP) tool that reduces words to their base forms, also known as lemmas. This process is called lemmatization. Lemmatization is similar to stemming, but it is more sophisticated and takes into account the context of the word to determine the correct lemma.\n",
    "\n",
    "For example, the lemma of the word \"running\" is \"run\". This is because \"running\" is a verb form of the word \"run\". Similarly, the lemma of the word \"saw\" is \"see\". This is because \"saw\" is a past tense form of the word \"see\".\"\"\"\n",
    "def lemmatizer(string):\n",
    "    word_pos_tags = nltk.pos_tag(word_tokenize(string)) # Get position tags\n",
    "    a=[wl.lemmatize(tag[0], get_wordnet_pos(tag[1])) for idx, tag in enumerate(word_pos_tags)] # Map the position tag and lemmatize the word/token\n",
    "    #print (\"\\n\\n\\n\\n\" ,a, \"\\n\\n\\n\\n\")\n",
    "    return \" \".join(a)\n",
    "def finalpreprocess(string):\n",
    "    return lemmatizer(stopword(preprocess(string)))\n",
    "if __name__ == \"__main__\":\n",
    "    #print(train_dataframe.head)\n",
    "    EDA(dataframe,train_dataframe,test_dataframe)\n",
    "#     train_dataframe['clean_text'] = train_dataframe['Content'].apply(lambda x: finalpreprocess(x))\n",
    "#     #print(train_dataframe.head)\n",
    "#     train_dataframe.to_csv('testclean.csv',sep='\\t',index=False)\n",
    "#     train_dataframe['word_count'] = train_dataframe['clean_text'].apply(lambda x: len(str(x).split()))\n",
    "#     #print(dataframe.head)\n",
    "#     print(\"\\nMean word count of cleaned Football Article: \",train_dataframe[train_dataframe['Category']=='business']['word_count'].mean()) #Disaster tweets\n",
    "#     print(\"Mean word count of cleaned Business Article: \",train_dataframe[train_dataframe['Category']=='entertainment']['word_count'].mean()) #Non-Disaster tweets\n",
    "#     print(\"Mean word count of cleaned Politics Article: \",train_dataframe[train_dataframe['Category']=='politics']['word_count'].mean())\n",
    "#     print(\"Mean word count of cleaned Film Article: \",train_dataframe[train_dataframe['Category']=='sport']['word_count'].mean())\n",
    "#     print(\"Mean word count of cleaned Technology Article: \",train_dataframe[train_dataframe['Category']=='tech']['word_count'].mean())\n",
    "    \n",
    "    \n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4a76e5ba-a79e-482a-af47-7c0446ecc02f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: newspaper3k in /Users/sathyax09/anaconda3/envs/python_project/lib/python3.10/site-packages (0.2.8)\n",
      "Requirement already satisfied: beautifulsoup4>=4.4.1 in /Users/sathyax09/anaconda3/envs/python_project/lib/python3.10/site-packages (from newspaper3k) (4.12.2)\n",
      "Requirement already satisfied: Pillow>=3.3.0 in /Users/sathyax09/anaconda3/envs/python_project/lib/python3.10/site-packages (from newspaper3k) (10.0.1)\n",
      "Requirement already satisfied: PyYAML>=3.11 in /Users/sathyax09/anaconda3/envs/python_project/lib/python3.10/site-packages (from newspaper3k) (6.0.1)\n",
      "Requirement already satisfied: cssselect>=0.9.2 in /Users/sathyax09/anaconda3/envs/python_project/lib/python3.10/site-packages (from newspaper3k) (1.2.0)\n",
      "Requirement already satisfied: lxml>=3.6.0 in /Users/sathyax09/anaconda3/envs/python_project/lib/python3.10/site-packages (from newspaper3k) (4.9.3)\n",
      "Requirement already satisfied: nltk>=3.2.1 in /Users/sathyax09/anaconda3/envs/python_project/lib/python3.10/site-packages (from newspaper3k) (3.8.1)\n",
      "Requirement already satisfied: requests>=2.10.0 in /Users/sathyax09/anaconda3/envs/python_project/lib/python3.10/site-packages (from newspaper3k) (2.31.0)\n",
      "Requirement already satisfied: feedparser>=5.2.1 in /Users/sathyax09/anaconda3/envs/python_project/lib/python3.10/site-packages (from newspaper3k) (6.0.10)\n",
      "Requirement already satisfied: tldextract>=2.0.1 in /Users/sathyax09/anaconda3/envs/python_project/lib/python3.10/site-packages (from newspaper3k) (5.1.1)\n",
      "Requirement already satisfied: feedfinder2>=0.0.4 in /Users/sathyax09/anaconda3/envs/python_project/lib/python3.10/site-packages (from newspaper3k) (0.0.4)\n",
      "Requirement already satisfied: jieba3k>=0.35.1 in /Users/sathyax09/anaconda3/envs/python_project/lib/python3.10/site-packages (from newspaper3k) (0.35.1)\n",
      "Requirement already satisfied: python-dateutil>=2.5.3 in /Users/sathyax09/anaconda3/envs/python_project/lib/python3.10/site-packages (from newspaper3k) (2.8.2)\n",
      "Requirement already satisfied: tinysegmenter==0.3 in /Users/sathyax09/anaconda3/envs/python_project/lib/python3.10/site-packages (from newspaper3k) (0.3)\n",
      "Requirement already satisfied: soupsieve>1.2 in /Users/sathyax09/anaconda3/envs/python_project/lib/python3.10/site-packages (from beautifulsoup4>=4.4.1->newspaper3k) (2.5)\n",
      "Requirement already satisfied: six in /Users/sathyax09/anaconda3/envs/python_project/lib/python3.10/site-packages (from feedfinder2>=0.0.4->newspaper3k) (1.16.0)\n",
      "Requirement already satisfied: sgmllib3k in /Users/sathyax09/anaconda3/envs/python_project/lib/python3.10/site-packages (from feedparser>=5.2.1->newspaper3k) (1.0.0)\n",
      "Requirement already satisfied: click in /Users/sathyax09/anaconda3/envs/python_project/lib/python3.10/site-packages (from nltk>=3.2.1->newspaper3k) (8.1.7)\n",
      "Requirement already satisfied: joblib in /Users/sathyax09/anaconda3/envs/python_project/lib/python3.10/site-packages (from nltk>=3.2.1->newspaper3k) (1.2.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /Users/sathyax09/anaconda3/envs/python_project/lib/python3.10/site-packages (from nltk>=3.2.1->newspaper3k) (2023.10.3)\n",
      "Requirement already satisfied: tqdm in /Users/sathyax09/anaconda3/envs/python_project/lib/python3.10/site-packages (from nltk>=3.2.1->newspaper3k) (4.65.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/sathyax09/anaconda3/envs/python_project/lib/python3.10/site-packages (from requests>=2.10.0->newspaper3k) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/sathyax09/anaconda3/envs/python_project/lib/python3.10/site-packages (from requests>=2.10.0->newspaper3k) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/sathyax09/anaconda3/envs/python_project/lib/python3.10/site-packages (from requests>=2.10.0->newspaper3k) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/sathyax09/anaconda3/envs/python_project/lib/python3.10/site-packages (from requests>=2.10.0->newspaper3k) (2023.11.17)\n",
      "Requirement already satisfied: requests-file>=1.4 in /Users/sathyax09/anaconda3/envs/python_project/lib/python3.10/site-packages (from tldextract>=2.0.1->newspaper3k) (1.5.1)\n",
      "Requirement already satisfied: filelock>=3.0.8 in /Users/sathyax09/anaconda3/envs/python_project/lib/python3.10/site-packages (from tldextract>=2.0.1->newspaper3k) (3.13.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install newspaper3k\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8da0744b-340e-49cb-9703-ec67b094bf8f",
   "metadata": {},
   "source": [
    "### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cfca421e-29d6-4033-a9ef-2b0b8bd20e23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting package metadata (current_repodata.json): done\n",
      "Solving environment: done\n",
      "\n",
      "\n",
      "==> WARNING: A newer version of conda exists. <==\n",
      "  current version: 23.7.4\n",
      "  latest version: 23.11.0\n",
      "\n",
      "Please update conda by running\n",
      "\n",
      "    $ conda update -n base -c defaults conda\n",
      "\n",
      "Or to minimize the number of packages updated during conda update use\n",
      "\n",
      "     conda install conda=23.11.0\n",
      "\n",
      "\n",
      "\n",
      "## Package Plan ##\n",
      "\n",
      "  environment location: /Users/sathyax09/anaconda3/envs/python_project\n",
      "\n",
      "  added / updated specs:\n",
      "    - lxml\n",
      "\n",
      "\n",
      "The following packages will be downloaded:\n",
      "\n",
      "    package                    |            build\n",
      "    ---------------------------|-----------------\n",
      "    ca-certificates-2023.08.22 |       hca03da5_0         131 KB  anaconda\n",
      "    certifi-2023.11.17         |  py310hca03da5_0         160 KB  anaconda\n",
      "    icu-73.1                   |       h313beb8_0        27.1 MB  anaconda\n",
      "    libiconv-1.16              |       h1a28f6b_2         1.3 MB  anaconda\n",
      "    libxml2-2.10.4             |       h0dcf63f_1         628 KB  anaconda\n",
      "    libxslt-1.1.37             |       h80987f9_1         236 KB  anaconda\n",
      "    lxml-4.9.3                 |  py310h50ffb84_0         1.4 MB  anaconda\n",
      "    openssl-3.0.12             |       h1a28f6b_0         4.5 MB  anaconda\n",
      "    ------------------------------------------------------------\n",
      "                                           Total:        35.5 MB\n",
      "\n",
      "The following NEW packages will be INSTALLED:\n",
      "\n",
      "  icu                anaconda/osx-arm64::icu-73.1-h313beb8_0 \n",
      "  libiconv           anaconda/osx-arm64::libiconv-1.16-h1a28f6b_2 \n",
      "  libxml2            anaconda/osx-arm64::libxml2-2.10.4-h0dcf63f_1 \n",
      "  libxslt            anaconda/osx-arm64::libxslt-1.1.37-h80987f9_1 \n",
      "  lxml               anaconda/osx-arm64::lxml-4.9.3-py310h50ffb84_0 \n",
      "\n",
      "The following packages will be SUPERSEDED by a higher-priority channel:\n",
      "\n",
      "  ca-certificates                                 pkgs/main --> anaconda \n",
      "  certifi                                         pkgs/main --> anaconda \n",
      "  openssl                                         pkgs/main --> anaconda \n",
      "\n",
      "\n",
      "\n",
      "Downloading and Extracting Packages\n",
      "lxml-4.9.3           | 1.4 MB    |                                       |   0% \n",
      "ca-certificates-2023 | 131 KB    |                                       |   0% \u001b[A\n",
      "\n",
      "libxml2-2.10.4       | 628 KB    |                                       |   0% \u001b[A\u001b[A\n",
      "\n",
      "\n",
      "libiconv-1.16        | 1.3 MB    |                                       |   0% \u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "openssl-3.0.12       | 4.5 MB    |                                       |   0% \u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "certifi-2023.11.17   | 160 KB    |                                       |   0% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "icu-73.1             | 27.1 MB   |                                       |   0% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "libxslt-1.1.37       | 236 KB    |                                       |   0% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "openssl-3.0.12       | 4.5 MB    | 1                                     |   0% \u001b[A\u001b[A\u001b[A\u001b[A\n",
      "ca-certificates-2023 | 131 KB    | ####5                                 |  12% \u001b[A\n",
      "\n",
      "\n",
      "libiconv-1.16        | 1.3 MB    | 4                                     |   1% \u001b[A\u001b[A\u001b[A\n",
      "ca-certificates-2023 | 131 KB    | ##################################### | 100% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "openssl-3.0.12       | 4.5 MB    | ####8                                 |  13% \u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "certifi-2023.11.17   | 160 KB    | ###6                                  |  10% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "libiconv-1.16        | 1.3 MB    | ##################4                   |  50% \u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "certifi-2023.11.17   | 160 KB    | ##################################### | 100% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "lxml-4.9.3           | 1.4 MB    | 4                                     |   1% \u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "lxml-4.9.3           | 1.4 MB    | ##############2                       |  38% \u001b[A\u001b[A\n",
      "\n",
      "libxml2-2.10.4       | 628 KB    | ###########################3          |  74% \u001b[A\u001b[A\n",
      "\n",
      "\n",
      "libiconv-1.16        | 1.3 MB    | ##################################### | 100% \u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "libiconv-1.16        | 1.3 MB    | ##################################### | 100% \u001b[A\u001b[A\u001b[A\n",
      "\n",
      "libxml2-2.10.4       | 628 KB    | ##################################### | 100% \u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "lxml-4.9.3           | 1.4 MB    | ##################################### | 100% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "icu-73.1             | 27.1 MB   |                                       |   0% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "libxslt-1.1.37       | 236 KB    | ##################################### | 100% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "openssl-3.0.12       | 4.5 MB    | ##################################### | 100% \u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "openssl-3.0.12       | 4.5 MB    | ##################################### | 100% \u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "icu-73.1             | 27.1 MB   | ##8                                   |   8% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "icu-73.1             | 27.1 MB   | ######1                               |  17% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "icu-73.1             | 27.1 MB   | #########2                            |  25% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "icu-73.1             | 27.1 MB   | ###############1                      |  41% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "icu-73.1             | 27.1 MB   | ##################9                   |  51% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "icu-73.1             | 27.1 MB   | #######################2              |  63% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "icu-73.1             | 27.1 MB   | ###########################1          |  73% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "icu-73.1             | 27.1 MB   | ##############################3       |  82% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "icu-73.1             | 27.1 MB   | #################################1    |  89% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "icu-73.1             | 27.1 MB   | ##################################### | 100% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "                                                                                \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\n",
      "                                                                                \u001b[A\u001b[A\n",
      "\n",
      "\n",
      "                                                                                \u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "                                                                                \u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "                                                                                \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "                                                                                \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "                                                                                \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "Preparing transaction: done\n",
      "Verifying transaction: done\n",
      "Executing transaction: done\n",
      "\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "conda install -c anaconda lxml\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "01b8cdf3-eaab-4b6a-88a3-706113811259",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/sathyax09/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8921d57d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from /Users/sathyax09/anaconda3/envs/python_project/lib/python3.10/site-packages/jieba/dict.txt ...\n",
      "Dumping model to file cache /var/folders/r0/47cv0h0s6nxcvwq4gkybtyy40000gn/T/jieba.cache\n",
      "Loading model cost 0.34336161613464355 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    },
    {
     "ename": "ArticleException",
     "evalue": "Article `download()` failed with 403 Client Error: Forbidden for url: https://www.cnn.com/cnn-underscored/deals/best-online-sales-right-now-2023-12-04 on URL https://www.cnn.com/cnn-underscored/deals/best-online-sales-right-now-2023-12-04",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mArticleException\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 29\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m article \u001b[38;5;129;01min\u001b[39;00m source\u001b[38;5;241m.\u001b[39marticles:\n\u001b[1;32m     28\u001b[0m     article\u001b[38;5;241m.\u001b[39mdownload()\n\u001b[0;32m---> 29\u001b[0m     \u001b[43marticle\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     30\u001b[0m     article\u001b[38;5;241m.\u001b[39mnlp()\n\u001b[1;32m     32\u001b[0m     \u001b[38;5;66;03m# Extract article information\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/python_project/lib/python3.10/site-packages/newspaper/article.py:191\u001b[0m, in \u001b[0;36mArticle.parse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    190\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mparse\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 191\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mthrow_if_not_downloaded_verbose\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    193\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdoc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget_parser()\u001b[38;5;241m.\u001b[39mfromstring(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhtml)\n\u001b[1;32m    194\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclean_doc \u001b[38;5;241m=\u001b[39m copy\u001b[38;5;241m.\u001b[39mdeepcopy(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdoc)\n",
      "File \u001b[0;32m~/anaconda3/envs/python_project/lib/python3.10/site-packages/newspaper/article.py:531\u001b[0m, in \u001b[0;36mArticle.throw_if_not_downloaded_verbose\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    529\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ArticleException(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mYou must `download()` an article first!\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    530\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdownload_state \u001b[38;5;241m==\u001b[39m ArticleDownloadState\u001b[38;5;241m.\u001b[39mFAILED_RESPONSE:\n\u001b[0;32m--> 531\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ArticleException(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mArticle `download()` failed with \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m on URL \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[1;32m    532\u001b[0m           (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdownload_exception_msg, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39murl))\n",
      "\u001b[0;31mArticleException\u001b[0m: Article `download()` failed with 403 Client Error: Forbidden for url: https://www.cnn.com/cnn-underscored/deals/best-online-sales-right-now-2023-12-04 on URL https://www.cnn.com/cnn-underscored/deals/best-online-sales-right-now-2023-12-04"
     ]
    }
   ],
   "source": [
    "import newspaper\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# List of URLs from different news sources with categories\n",
    "urls_to_scrape = [\n",
    "    {\"url\": \"https://www.bbc.co.uk\", \"category\": \"News\"},\n",
    "    {\"url\": \"https://www.nytimes.com\", \"category\": \"Politics\"},\n",
    "    {\"url\": \"https://www.nationalgeographic.com\", \"category\": \"Science\"},\n",
    "    {\"url\": \"https://www.indiatimes.com\", \"category\": \"World\"},\n",
    "    {\"url\": \"https://edition.cnn.com\", \"category\": \"Global\"},\n",
    "    # Add more URLs along with their categories\n",
    "]\n",
    "\n",
    "# Initialize an empty DataFrame to store the articles\n",
    "final_df = pd.DataFrame(columns=['Title', 'Authors', 'Text', 'Summary', 'published_date', 'Source', 'Category'])\n",
    "\n",
    "articles_count = 0\n",
    "for item in urls_to_scrape:\n",
    "    url = item[\"url\"]\n",
    "    category = item[\"category\"]\n",
    "    \n",
    "    # Build newspaper from each URL\n",
    "    source = newspaper.build(url, memoize_articles=False)\n",
    "    \n",
    "    # Loop through articles in each source\n",
    "    for article in source.articles:\n",
    "        article.download()\n",
    "        article.parse()\n",
    "        article.nlp()\n",
    "\n",
    "        # Extract article information\n",
    "        temp_df = pd.DataFrame(columns=['Title', 'Authors', 'Text', 'Summary', 'published_date', 'Source', 'Category'])\n",
    "        temp_df.at[0, 'Authors'] = article.authors\n",
    "        temp_df.at[0, 'Title'] = article.title\n",
    "        temp_df.at[0, 'Text'] = article.text\n",
    "        temp_df.at[0, 'Summary'] = article.summary\n",
    "        temp_df.at[0, 'published_date'] = article.publish_date\n",
    "        temp_df.at[0, 'Source'] = article.source_url\n",
    "        temp_df.at[0, 'Category'] = category  # Assigning the category\n",
    "\n",
    "        # Append the article data to the final DataFrame\n",
    "        final_df = pd.concat([final_df, temp_df], ignore_index=True)\n",
    "        \n",
    "        articles_count += 1\n",
    "        \n",
    "        # Check if the number of articles reaches 1000, then break the loop\n",
    "        if articles_count >= 1000:\n",
    "            break\n",
    "    \n",
    "    # Check if the number of articles reaches 1000, then break the outer loop\n",
    "    if articles_count >= 1000:\n",
    "        break\n",
    "\n",
    "# Export the Pandas DataFrame to a CSV file\n",
    "final_df.to_csv('my_scraped_articles_1k.csv')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dc719eb-729b-4c2b-8499-2abbdddf17d6",
   "metadata": {},
   "source": [
    "# scrapped some articles from different sources. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b45eb862-6d36-4e39-9390-dd7dd0dc729f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
